model: Generic_TransUNet_max_ppbp  # model name
model_params:  # variants
  is_max_bottleneck_transformer: False  # TransUNet backbone

  is_masked_attn: True  # turn on Transformer decoder
  max_dec_layers: 10  # number of Transformer decoder layers
  vit_depth: 1  # number of Transformer layer in TransUNet
  max_msda: ''
  is_max_ms: True  # using UNet multi-scale feature to update query in Transformer decoder
  max_ms_idxs: [-2,-1]  # which scale feature
  max_hidden_dim: 256
  mw: 1.0  # loss only applied onto Transformer decoder, istead of UNet decoder.
  is_max_ds: True  # deep-supervision in Transformer decoder
  is_masking: True  # use masked-attention
  is_max_hungarian: True  # turn on hungarian matching
  num_queries: 4
  is_max_cls: True  # turn on mask classification, along with hungarian matching
  is_mhsa_float32: False  # turn on float32 (rather than fp16) incase NAN in softmax
  decoder_layer_scale: False


max_loss_cal: 'v1'
disable_ds: True
initial_lr: 0.0001
lrschedule: warmup_cosine
resume: 'local_latest'
warmup_epochs: 10
max_num_epochs: 2000
task: Task301_clinic
network: 3d_fullres
network_trainer: nnUNetTrainerV2
hdfs_base: Task301_clinic_3DTransUNet_decoder_only

