model: Generic_TransUNet_max_ppbp
model_params:
  # these two must be set together
  is_max: False  # disable decoder
  is_max_hungarian: False # turn off hungarian matching

  is_masked_attn: True  # turn on Transformer decoder
  max_dec_layers: 10  # number of Transformer decoder layers
  is_max_bottleneck_transformer: True  # TransUNet backbone
  vit_depth: 12  # number of Transformer layer in TransUNet
  max_msda: ''
  is_max_ms: True  # using UNet multi-scale feature to update query in Transformer decoder
  max_ms_idxs: [-4, -3, -2]  # which scale feature
  max_hidden_dim: 256
  mw: 1.0  # loss only applied onto Transformer decoder, istead of UNet decoder.
  is_max_ds: True  # deep-supervision in Transformer decoder
  is_masking: True  # use masked-attention
  num_queries: 4
  is_max_cls: True  # turn on mask classification, along with hungarian matching
  is_mhsa_float32: False  # if turn on float32 (rather than fp16) incase NAN in softmax
  is_vit_pretrain: True
  vit_layer_scale: True

max_loss_cal: 'v1'
initial_lr: 0.0001
lrschedule: warmup_cosine
resume: 'local_latest'
warmup_epochs: 10
max_num_epochs: 500
task: Task302_ACDCfaster
network: 3d_fullres
network_trainer: nnUNetTrainerV2
hdfs_base: ACDCfaster_3DTransUNet_encoder_only